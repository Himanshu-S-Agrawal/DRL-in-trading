{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating synthetic alphas : we can process any input data with open prices and generate desired stochastically constant correlation or trend based correlation alphas. Stochastically constant correlation can even be directly tested on our pre-trained policy which has been trained on 5 min data frequency, and has showcased generalized results for US tech sector stocks. If your data frequency is of different timespan I would suggest training your own policy which also takes only few steps as you follow this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from generate_alpha.trend_alpha import process_data  if you want to generate trend based alpha\n",
    "from generate_alpha.generate_alphav3 import process_data   #if you want to generate SCC alpha\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('data/ORCL-5min-2020-2023.parquet', engine='pyarrow')\n",
    "# Ensure the index is in datetime format\n",
    "df.index = pd.to_datetime(df.index)\n",
    "# Filter out rows based on time\n",
    "df = df.between_time('09:30:00', '16:00:00')   \n",
    "# Add the ret and fwd_ret5 columns\n",
    "# Calculate ret and fwd_ret5 day by day\n",
    "df['ret'] = df.groupby(df.index.date)['Open'].pct_change()\n",
    "df['fwd_ret5'] = df.groupby(df.index.date)['Open'].pct_change(periods=-5)\n",
    "df.dropna(inplace=True)\n",
    "df_new = df.drop(columns=['High', 'Low', 'Close', 'Volume'])\n",
    "df_final = process_data(df_new, name_to_save='data/ORCL_SCC.csv', desired_correlation=0.1, save_to_csv=True)\n",
    "#there is correlation loss as we go day by day and are not processing the entire data at once whereas correlation calculated is for entire data not day by day \n",
    "# can expect around 1 to 1.5 % correlation less compared to desired correlation input "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to split your data into train, validate and test datasets. Code below will do that and save csv files for all three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('data/IBM_trend.csv')\n",
    "\n",
    "# Split the data into an 80-20 ratio\n",
    "train_data, x_data = train_test_split(df, train_size=0.7, shuffle=False)\n",
    "validate_data, test_data = train_test_split(x_data, train_size=0.5, shuffle=False)\n",
    "\n",
    "# Save the train and test data to separate CSV files\n",
    "train_data.to_csv('data/IBMtrendtrain_data.csv', index=False)\n",
    "test_data.to_csv('data/IBMtrendtest_data.csv', index=False)\n",
    "validate_data.to_csv('data/IBMtrendvalidation_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Model Training\n",
    "\n",
    "The training process for our RL model is structured into the following key steps:\n",
    "\n",
    "## 1. **Ray Instance Initialization**\n",
    "Initialize the Ray framework to manage distributed training.\n",
    "\n",
    "## 2. **Environment Registration**\n",
    "Register our custom trading environment to make it accessible for Ray's RLlib.\n",
    "\n",
    "## 3. **Data Path Setup**\n",
    "Determine the paths for training and validation datasets.\n",
    "\n",
    "## 4. **Training with PPO and Ray Tune (PBT)**\n",
    "Leverage Proximal Policy Optimization (PPO) for training, and use Population Based Training (PBT) via Ray Tune for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import EnvCompatibility\n",
    "from ray.tune.registry import register_env\n",
    "#We currently have 2 different simple trading environments one for SCC and one for trend \n",
    "from custom_env.simpletrading_env import TradingEnv\n",
    "#from custom_env.trendtrading_env import TradingEnv  #use this environment when training on trend based data\n",
    "\n",
    "def create_compatible_trading_env(env_config):\n",
    "    env = TradingEnv(env_config)\n",
    "    return EnvCompatibility(env)\n",
    "\n",
    "register_env(\"wrapped_trading_env\", create_compatible_trading_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Construct the absolute paths of the data files\n",
    "train_data_path = os.path.join(cwd, 'data/IBMtrendtrain_data.csv')\n",
    "validation_data_path = os.path.join(cwd, 'data/IBMtrendvalidation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ray\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from custom_stopper import EarlyStoppingStopper\n",
    "import numpy as np\n",
    "\n",
    "# Define a PBT scheduler\n",
    "pbt = PopulationBasedTraining(\n",
    "    time_attr=\"time_total_s\",\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=50.0, # This defines how frequently (in seconds) to perturb hyperparameters.\n",
    "    hyperparam_mutations={\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-3),\n",
    "        \"gamma\": tune.uniform(0.95, 0.99),\n",
    "        \"clip_param\": tune.uniform(0.1, 0.3),\n",
    "        \"kl_coeff\" : tune.uniform(0.2, 1.5)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define a configuration\n",
    "config = {\n",
    "    \"env\": \"wrapped_trading_env\",\n",
    "    \"env_config\": {\n",
    "        \"data_filepath\": train_data_path,\n",
    "        #\"window_size\": 5,\n",
    "        \"least_episode_size\": 75\n",
    "    },\n",
    "    \"evaluation_interval\": 100,\n",
    "    \"evaluation_duration\": 2,\n",
    "    \"evaluation_parallel_to_training\": True,\n",
    "    \"evaluation_config\": {\n",
    "        \"env\": \"wrapped_trading_env\",\n",
    "        \"env_config\": {\n",
    "        \"data_filepath\": validation_data_path,  # replace with your validation data\n",
    "        #\"window_size\": 5,\n",
    "        \"least_episode_size\": 75\n",
    "        },\n",
    "        \"explore\": False,\n",
    "    },\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "    \"rollout_fragment_length\": 'auto',\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"always_attach_evaluation_results\": True,\n",
    "    \"num_workers\": 2,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"framework\" : 'torch',\n",
    "    \"num_gpus\": 0,\n",
    "    \"shuffle_sequences\": False,\n",
    "    \"vf_loss_coeff\": 0, \n",
    "    \"lr\": 0.0003,\n",
    "    \"gamma\": 0.79,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"kl_coeff\": 0.5,\n",
    "    \"num_sgd_iter\" : tune.randint(25, 35),\n",
    "    \"sgd_minibatch_size\" : tune.sample_from(lambda _: np.random.randint(50, 150)), #this should be set depending on data frequency, our data frequency is 5 min, and we would like it to be around 1 day\n",
    "    \"train_batch_size\": 2250,  #this should be set depending on data frequency, our data frequency is 5 mins, so around a month we have set it, shouldn't be very large otherwise can affect training\n",
    "   #can set this model accoding to user preference for SCC we used only Dense layers and provided previous 5 time steps of state values\n",
    "   #We are using LSTM for trend_alpha while also providing only current_Step state values\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [128, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    
    "    \"log_level\": \"WARNING\"\n",
    "}\n",
    "stopper = EarlyStoppingStopper(patience=100, eval_patience=100, min_iterations=500)\n",
    "\n",
    "# Run the Tune experiment\n",
    "analysis = tune.run(\n",
    "    PPO,\n",
    "    name=\"PPO_PBT_Trading\",\n",
    "    #reuse_actors=True,\n",
    "    scheduler=pbt,\n",
    "    #stop={\n",
    "    #    \"episode_reward_mean\": 0.15,\n",
    "    #},\n",
    "    stop = stopper,\n",
    "    config=config,\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_freq=25,\n",
    "    num_samples=2,\n",
    "    verbose=1\n",
    "    #local_dir='/home/himanshu/ray_results/'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quick analyse the generated training results below. For indepth analysis I would recommend trying tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe for all the trials\n",
    "df = analysis.results_df\n",
    "\n",
    "# Get the trial with the highest mean reward\n",
    "best_trial = analysis.get_best_trial(\"episode_reward_mean\", \"max\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final reward:\", best_trial.last_result[\"episode_reward_mean\"])\n",
    "print(df['evaluation/episode_reward_mean'])\n",
    "best_trial_evaluation_results = best_trial.last_result[\"evaluation\"]\n",
    "print(best_trial_evaluation_results[\"episode_reward_mean\"])\n",
    "best_checkpoint = analysis.get_best_checkpoint(best_trial, metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(best_checkpoint) # u will get the checkpoint_path_to_save from best_checkpoint\n",
    "# can make new cells for any further anaylsis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is upto mark or you would like to use the model for further testing or as a pre-trained model to further improve. DO save the model into a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Extract the checkpoint path from the best_checkpoint object\n",
    "#for example Checkpoint(local_path=/Users/himanshuagrawal/ray_results/PPO_PBT_Trading/PPO_wrapped_trading_env_76be2_00000_0_num_sgd_iter=43_2023-09-19_19-27-25/checkpoint_003580)\n",
    "checkpoint_path_to_save = '/Users/himanshuagrawal/ray_results/PPO_PBT_Trading/PPO_wrapped_trading_env_05cda_00000_0_num_sgd_iter=29_2023-10-19_15-28-15/checkpoint_000325'\n",
    "\n",
    "# Save the checkpoint path and best trial config\n",
    "with open('model_user.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'checkpoint_path': checkpoint_path_to_save,\n",
    "        'best_trial_config': best_trial.config\n",
    "    }, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes 2nd part where we can test any model json's on any data defined , further can make cells to analyse the results and also save the results in a json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing.trend_test import test_trained_model #for testing trend policies\n",
    "#from testing.test import test_trained_model  for testing SCC policy \n",
    "#For running test ensure to edit the json file to have correct checkpoint path currently it has my local path,\n",
    "#Replace my local path with yours in the model_info1.json, although model config is not neccesary for testing it's good to know config's that work\n",
    "rewards, results = test_trained_model(model_info_path='results/model_info1.json', data_filepath='data/IBMtest_data.csv', num_episodes=50)\n",
    "# Analyze the results or plot graphs as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def default_serialize(o):\n",
    "    if isinstance(o, np.integer):\n",
    "        return int(o)\n",
    "    elif isinstance(o, np.floating):\n",
    "        return float(o)\n",
    "    elif isinstance(o, np.ndarray):\n",
    "        return o.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type '{type(o).__name__}' is not JSON serializable\")\n",
    "\n",
    "with open('results_user.json', 'w') as f:\n",
    "    json.dump(results, f, default=default_serialize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse tested data results can use our results_analysis.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results_analysis import analyze_results\n",
    "\n",
    "df = analyze_results('results_user.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
