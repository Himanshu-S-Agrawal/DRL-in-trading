{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating synthetic alphas : we can process any input data with open prices and generate a random variable that has desired correlation with forward 5 time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_alpha.generate_alphav3 import process_data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('data/IBM-5min-2020-2023.parquet', engine='pyarrow')\n",
    "# Ensure the index is in datetime format\n",
    "df.index = pd.to_datetime(df.index)\n",
    "    # Filter out rows based on time\n",
    "df = df.between_time('09:30:00', '16:00:00')   \n",
    "    # Add the ret and fwd_ret5 columns\n",
    "# Calculate ret and fwd_ret5 day by day\n",
    "df['ret'] = df.groupby(df.index.date)['Open'].pct_change()\n",
    "df['fwd_ret5'] = df.groupby(df.index.date)['Open'].pct_change(periods=-5)\n",
    "df_new = df.drop(columns=['High', 'Low', 'Close', 'Volume'])\n",
    "df_final = process_data(df_new, name_to_save='data/IBM_SCC.csv', desired_correlation=0.1, save_to_csv=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to split your data into train, validate and test datasets. Code below will do that and save csv files for all three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "#df = pd.read_csv('data/df_final.csv')\n",
    "\n",
    "# Convert 'Unnamed: 0' to datetime\n",
    "df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'])\n",
    "\n",
    "# Filter out rows with the year 2020\n",
    "df = df[df['Unnamed: 0'].dt.year != 2020]\n",
    "# Split the data into an 80-20 ratio\n",
    "train_data, x_data = train_test_split(df, train_size=0.7, shuffle=False)\n",
    "validate_data, test_data = train_test_split(x_data, train_size=0.5, shuffle=False)\n",
    "\n",
    "# Save the train and test data to separate CSV files\n",
    "train_data.to_csv('data/IBMtrain_data.csv', index=False)\n",
    "test_data.to_csv('data/IBMtest_data.csv', index=False)\n",
    "validate_data.to_csv('data/IBMvalidation_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Model Training\n",
    "\n",
    "The training process for our RL model is structured into the following key steps:\n",
    "\n",
    "## 1. **Ray Instance Initialization**\n",
    "Initialize the Ray framework to manage distributed training.\n",
    "\n",
    "## 2. **Environment Registration**\n",
    "Register our custom trading environment to make it accessible for Ray's RLlib.\n",
    "\n",
    "## 3. **Data Path Setup**\n",
    "Determine the paths for training and validation datasets.\n",
    "\n",
    "## 4. **Training with PPO and Ray Tune (PBT)**\n",
    "Leverage Proximal Policy Optimization (PPO) for training, and use Population Based Training (PBT) via Ray Tune for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import EnvCompatibility\n",
    "from ray.tune.registry import register_env\n",
    "from simpletrading_env import TradingEnv\n",
    "\n",
    "def create_compatible_trading_env(env_config):\n",
    "    env = TradingEnv(env_config)\n",
    "    return EnvCompatibility(env)\n",
    "\n",
    "register_env(\"wrapped_trading_env\", create_compatible_trading_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Construct the absolute paths of the data files\n",
    "train_data_path = os.path.join(cwd, 'data/IBMtrain_data.csv')\n",
    "validation_data_path = os.path.join(cwd, 'data/IBMvalidation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ray\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from custom_stopper import EarlyStoppingStopper\n",
    "import numpy as np\n",
    "\n",
    "# Define a PBT scheduler\n",
    "pbt = PopulationBasedTraining(\n",
    "    time_attr=\"time_total_s\",\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=50.0, # This defines how frequently (in seconds) to perturb hyperparameters.\n",
    "    hyperparam_mutations={\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-3),\n",
    "        \"gamma\": tune.uniform(0.95, 0.99),\n",
    "        \"clip_param\": tune.uniform(0.1, 0.3),\n",
    "        \"kl_coeff\" : tune.uniform(0.2, 1.5)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define a configuration\n",
    "config = {\n",
    "    \"env\": \"wrapped_trading_env\",\n",
    "    \"env_config\": {\n",
    "        \"data_filepath\": train_data_path,\n",
    "        \"window_size\": 5,\n",
    "        \"least_episode_size\": 75\n",
    "    },\n",
    "    \"evaluation_interval\": 100,\n",
    "    \"evaluation_duration\": 2,\n",
    "    \"evaluation_parallel_to_training\": True,\n",
    "    \"evaluation_config\": {\n",
    "        \"env\": \"wrapped_trading_env\",\n",
    "        \"env_config\": {\n",
    "        \"data_filepath\": validation_data_path,  # replace with your validation data\n",
    "        \"window_size\": 5,\n",
    "        \"least_episode_size\": 75\n",
    "        },\n",
    "        \"explore\": False,\n",
    "    },\n",
    "    \"batch_mode\": \"truncate_episodes\",\n",
    "    \"rollout_fragment_length\": 'auto',\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"always_attach_evaluation_results\": True,\n",
    "    \"num_workers\": 3,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"framework\" : 'torch',\n",
    "    \"num_gpus\": 0,\n",
    "    \"shuffle_sequences\": False,\n",
    "    \"vf_loss_coeff\": 0, \n",
    "    \"lr\": 0.0003,\n",
    "    \"gamma\": 0.99,\n",
    "    \"clip_param\": 0.2,\n",
    "    \"kl_coeff\": 0.5,\n",
    "    \"num_sgd_iter\" : tune.randint(25, 35),\n",
    "    \"sgd_minibatch_size\" : tune.sample_from(lambda _: np.random.randint(50, 150)),\n",
    "    \"train_batch_size\": 2250,\n",
    "    \"model\":{\n",
    "        #\"fcnet_hiddens\": [64, 32],\n",
    "        \"fcnet_hiddens\": [128, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "        \"use_lstm\": False,    \n",
    "    },\n",
    "    \"log_level\": \"WARNING\"\n",
    "}\n",
    "stopper = EarlyStoppingStopper(patience=100, eval_patience=100, min_iterations=500)\n",
    "\n",
    "# Run the Tune experiment\n",
    "analysis = tune.run(\n",
    "    PPO,\n",
    "    #resources_per_trial={\"cpu\": 1, \"gpu\": 0},\n",
    "    name=\"PPO_PBT_Trading\",\n",
    "    #reuse_actors=True,\n",
    "    scheduler=pbt,\n",
    "    #stop={\n",
    "    #    \"episode_reward_mean\": 0.15,\n",
    "    #},\n",
    "    stop = stopper,\n",
    "    config=config,\n",
    "    checkpoint_at_end=True,\n",
    "    checkpoint_freq=10,\n",
    "    num_samples=2,\n",
    "    verbose=1\n",
    "    #local_dir='/home/himanshu/ray_results/'\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quick analyse the generated training results below. For indepth analysis I would recommend trying tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe for all the trials\n",
    "df = analysis.results_df\n",
    "\n",
    "# Get the trial with the highest mean reward\n",
    "best_trial = analysis.get_best_trial(\"episode_reward_mean\", \"max\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final reward:\", best_trial.last_result[\"episode_reward_mean\"])\n",
    "print(df['evaluation/episode_reward_mean'])\n",
    "best_trial_evaluation_results = best_trial.last_result[\"evaluation\"]\n",
    "print(best_trial_evaluation_results[\"episode_reward_mean\"])\n",
    "best_checkpoint = analysis.get_best_checkpoint(best_trial, metric=\"episode_reward_mean\", mode=\"max\")\n",
    "print(best_checkpoint) # u will get the checkpoint_path_to_save from best_checkpoint\n",
    "# can make new cells for any further anaylsis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model is upto mark or you would like to use the model for further testing or as a pre-trained model to further improve. DO save the model into a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Extract the checkpoint path from the best_checkpoint object\n",
    "#for example Checkpoint(local_path=/Users/himanshuagrawal/ray_results/PPO_PBT_Trading/PPO_wrapped_trading_env_76be2_00000_0_num_sgd_iter=43_2023-09-19_19-27-25/checkpoint_003580)\n",
    "checkpoint_path_to_save = '/Users/himanshuagrawal/ray_results/PPO_PBT_Trading/PPO_wrapped_trading_env_c95c4_00001_1_num_sgd_iter=30_2023-10-06_12-07-14/checkpoint_000290'\n",
    "\n",
    "# Save the checkpoint path and best trial config\n",
    "with open('model_info1.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'checkpoint_path': checkpoint_path_to_save,\n",
    "        'best_trial_config': best_trial.config\n",
    "    }, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes 2nd part where we can test any model json's on any data defined , further can make cells to analyse the results and also save the results in a json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import test_trained_model\n",
    "\n",
    "rewards, results = test_trained_model(model_info_path='model_info1.json', data_filepath='data/IBMtest_data.csv', num_episodes=50)\n",
    "# Analyze the results or plot graphs as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def default_serialize(o):\n",
    "    if isinstance(o, np.integer):\n",
    "        return int(o)\n",
    "    elif isinstance(o, np.floating):\n",
    "        return float(o)\n",
    "    elif isinstance(o, np.ndarray):\n",
    "        return o.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type '{type(o).__name__}' is not JSON serializable\")\n",
    "\n",
    "with open('resultsIBM.json', 'w') as f:\n",
    "    json.dump(results, f, default=default_serialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
